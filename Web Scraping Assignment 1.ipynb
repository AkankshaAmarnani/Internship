{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7083478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f851ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2da9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "page1=requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "page1        #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup1= BeautifulSoup(page1.content)\n",
    "\n",
    "header_tag=[]        #Empty list to store header tags\n",
    "for i in soup1.find_all('h2',class_=\"mp-h2\"):\n",
    "    header_tag.append(i.text)\n",
    "    \n",
    "print(\"The header tags from wikipedia.org are- \")\n",
    "header_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de591a35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "page2=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\")  #Contains top 50 movies\n",
    "page2a=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt\")    #Contains corresponding top 50 movies\n",
    "\n",
    "page2          #As the response is 200, we can proceed with this url\n",
    "page2a         #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup2= BeautifulSoup(page2.content)\n",
    "soup2a= BeautifulSoup(page2a.content)\n",
    "\n",
    "name=[]            #Empty list to store movie names\n",
    "for i in soup2.find_all('h3',class_=\"lister-item-header\"):\n",
    "    l=len(i.text.split())\n",
    "    name.append(' '.join(i.text.split()[1:l-1]))          #To store only the movie name as a string\n",
    "for i in soup2a.find_all('h3',class_=\"lister-item-header\"):\n",
    "    l=len(i.text.split())\n",
    "    name.append(' '.join(i.text.split()[1:l-1]))\n",
    "\n",
    "rating=[]         #Empty list to store rating\n",
    "for i in soup2.find_all('div',class_=\"inline-block ratings-imdb-rating\"):\n",
    "    rating.append(' '.join(i.text.split()))               #To store only the rating as a string\n",
    "for i in soup2a.find_all('div',class_=\"inline-block ratings-imdb-rating\"):\n",
    "    rating.append(' '.join(i.text.split()))\n",
    "\n",
    "year=[]           #Empty list to store year of release\n",
    "for i in soup2.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    year.append(i.text.replace('(','').replace(')',''))    #To store only the year of release as a string\n",
    "for i in soup2a.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    year.append(i.text.replace('(','').replace(')',''))\n",
    "    \n",
    "s_no=[]\n",
    "for j in range(1,101):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)                #To display the entire dataframe\n",
    "df2=pd.DataFrame({'Name':name, 'Rating':rating, 'Year of Release': year}, index=s_no)\n",
    "print(\"IMDB’s Top rated 100 movies' data\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3689fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "page3=requests.get(\"https://www.imdb.com/india/top-rated-indian-movies/?sort=ir,desc&mode=simple&page=1\")  \n",
    "\n",
    "page3                 #As the response is 200, we can proceed with this url\n",
    "             \n",
    "soup3= BeautifulSoup(page3.content)\n",
    "\n",
    "a=1             #counters to keep top 100 indian movies\n",
    "b=1\n",
    "c=1\n",
    "\n",
    "name=[]            #Empty list to store movie names\n",
    "for i in soup3.find_all('td',class_=\"titleColumn\"):\n",
    "    l=len(i.text.split())\n",
    "    if a<=100:                                                #To store top 100 movies\n",
    "        name.append(' '.join(i.text.split()[1:l-1]))          #To store only the movie name as a string\n",
    "        a=a+1\n",
    "\n",
    "rating=[]         #Empty list to store rating\n",
    "for i in soup3.find_all('td',class_=\"ratingColumn imdbRating\"):\n",
    "    if b<=100:\n",
    "        rating.append(' '.join(i.text.split()))               #To store only the rating as a string\n",
    "        b=b+1\n",
    "\n",
    "year=[]           #Empty list to store year of release\n",
    "for i in soup3.find_all('span',class_=\"secondaryInfo\"):\n",
    "    if c<=100:\n",
    "        year.append(i.text.replace('(','').replace(')',''))    #To store only the year of release as a string\n",
    "        c=c+1\n",
    "        \n",
    "s_no=[]\n",
    "for j in range(1,101):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)                #To display the entire dataframe\n",
    "df3=pd.DataFrame({'Name':name, 'Rating':rating, 'Year of Release': year}, index=s_no)\n",
    "print(\"IMDB’s Top rated 100 movies' data\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083708e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. Write a python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm\n",
    "\n",
    "\n",
    "page4=requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "page4           #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup4= BeautifulSoup(page4.content)\n",
    "\n",
    "pres=[]                   #Empty list to store names of respected former presidents of India\n",
    "for i in soup4.find_all('div',class_=\"presidentListing\"):\n",
    "    l=len(i.text.split())\n",
    "    for j in range(0,l):\n",
    "        a=i.text.split()[j]\n",
    "        if a[0]=='(':\n",
    "            k=j\n",
    "            break\n",
    "    pres.append(' '.join(i.text.split()[0:k]))          #To store only the names of respected former presidents of India\n",
    "\n",
    "tf=[]\n",
    "z=1\n",
    "for i in soup4.find_all('div',class_=\"presidentListing\"):\n",
    "    l=len(i.text.split())\n",
    "    for j in range(0,l):\n",
    "        a=i.text.split()[j]\n",
    "        if a=='Office:':\n",
    "            k=j\n",
    "            break\n",
    "    if z<=3:\n",
    "        tf.append(' '.join(i.text.split()[k+1:l-1]))    #To store only the term of office of respected former presidents of India\n",
    "        z=z+1\n",
    "    else:\n",
    "        tf.append(' '.join(i.text.split()[k+1:l])) \n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df4=pd.DataFrame({'Name':pres, 'Term of Office':tf})\n",
    "print(\"The list of respected former presidents of India\")\n",
    "df4.set_index('Name', inplace=True)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f0ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5a. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. \n",
    "\n",
    "page5a=requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "page5a   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup5a= BeautifulSoup(page5a.content)\n",
    "\n",
    "a=soup5a.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "t=[]\n",
    "m=[]\n",
    "p=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(1,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5':\n",
    "        k=j\n",
    "        break\n",
    "t.append(' '.join(a.text.split()[1:k-1]))\n",
    "m.append(''.join(a.text.split()[k]))\n",
    "p.append(''.join(a.text.split()[k+1]))\n",
    "r.append(''.join(a.text.split()[k+2]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup5a.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(1,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5':\n",
    "                k=j\n",
    "                break\n",
    "        t.append(' '.join(i.text.split()[1:k-1]))\n",
    "        m.append(''.join(i.text.split()[k]))\n",
    "        p.append(''.join(i.text.split()[k+1]))\n",
    "        r.append(''.join(i.text.split()[k+2]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df5a=pd.DataFrame({'Team':t, 'Matches':m, 'Points':p, 'Ratings':r}, index=s_no)\n",
    "print(\"Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\")\n",
    "df5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8d8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5b. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "\n",
    "page5b=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "page5b   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup5b= BeautifulSoup(page5b.content)\n",
    "\n",
    "a=soup5b.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='1':\n",
    "        k=j\n",
    "        break\n",
    "p.append(' '.join(a.text.split()[2:k-1]))\n",
    "t.append(''.join(a.text.split()[k-1]))\n",
    "r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup5b.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='1':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df5b=pd.DataFrame({'Batsmen':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 ODI Batsmen along with the records of their team and rating.\")\n",
    "df5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d4082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5c. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "\n",
    "page5c=requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "page5c   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup5c= BeautifulSoup(page5c.content)\n",
    "\n",
    "a=soup5c.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='1':\n",
    "        k=j\n",
    "        break\n",
    "p.append(' '.join(a.text.split()[2:k-1]))\n",
    "t.append(''.join(a.text.split()[k-1]))\n",
    "r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup5c.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='1':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df5c=pd.DataFrame({'Bowler':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 ODI Bowlers along with the records of their team and rating.\")\n",
    "df5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6da4eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6a. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\n",
    "\n",
    "page6a=requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "page6a   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup6a= BeautifulSoup(page6a.content)\n",
    "\n",
    "a=soup6a.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "t=[]\n",
    "m=[]\n",
    "p=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(1,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "        k=j\n",
    "        break\n",
    "t.append(' '.join(a.text.split()[1:k-1]))\n",
    "m.append(''.join(a.text.split()[k]))\n",
    "p.append(''.join(a.text.split()[k+1]))\n",
    "r.append(''.join(a.text.split()[k+2]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup6a.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(1,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='1' or b[0]=='2' or b[0]=='3' or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "                k=j\n",
    "                break\n",
    "        t.append(' '.join(i.text.split()[1:k-1]))\n",
    "        m.append(''.join(i.text.split()[k]))\n",
    "        p.append(''.join(i.text.split()[k+1]))\n",
    "        r.append(''.join(i.text.split()[k+2]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df6a=pd.DataFrame({'Team':t, 'Matches':m, 'Points':p, 'Ratings':r}, index=s_no)\n",
    "print(\"Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\")\n",
    "df6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d0b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6b. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "\n",
    "page6b=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "page6b   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup6b= BeautifulSoup(page6b.content)\n",
    "\n",
    "a=soup6b.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='5' or b[0]=='4' or b[0]=='1':\n",
    "        k=j\n",
    "        break\n",
    "if a.text.split()[1]=='(0)':\n",
    "    p.append(' '.join(a.text.split()[2:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "else:\n",
    "    p.append(' '.join(a.text.split()[15:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup6b.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='9' or b[0]=='8' or b[0]=='7' or b[0]=='6' or b[0]=='5' or b[0]=='4' or b[0]=='1':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "    \n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df6b=pd.DataFrame({'Batting Player':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 women’s ODI Batting players along with the records of their team and rating.\")\n",
    "df6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5bb7d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#6c. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "page6c=requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "page6c   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup6c= BeautifulSoup(page6c.content)\n",
    "\n",
    "a=soup6c.find('tr',class_=\"rankings-block__banner\")    #Store the highest ranker\n",
    "p=[]\n",
    "t=[]\n",
    "r=[]\n",
    "l=len(a.text.split())\n",
    "for j in range(2,l):\n",
    "    b=a.text.split()[j]\n",
    "    if b[0]=='1' or b[0]=='2' or b[0]=='3'  or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "        k=j\n",
    "        break\n",
    "if a.text.split()[1]=='(0)':\n",
    "    p.append(' '.join(a.text.split()[2:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "else:\n",
    "    p.append(' '.join(a.text.split()[15:k-1]))\n",
    "    t.append(''.join(a.text.split()[k-1]))\n",
    "    r.append(''.join(a.text.split()[k]))\n",
    "\n",
    "z=1                                #Counter to get only top 10\n",
    "for i in soup6c.find_all('tr',class_=\"table-body\"):\n",
    "    if z<=9:\n",
    "        l=len(i.text.split())\n",
    "        for j in range(2,l):\n",
    "            b=i.text.split()[j]\n",
    "            if b[0]=='1' or b[0]=='2' or b[0]=='3'  or b[0]=='4' or b[0]=='5' or b[0]=='6' or b[0]=='7' or b[0]=='8' or b[0]=='9':\n",
    "                k=j\n",
    "                break\n",
    "        if i.text.split()[1]=='(0)':\n",
    "            p.append(' '.join(i.text.split()[2:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        else:\n",
    "            p.append(' '.join(i.text.split()[15:k-1]))\n",
    "            t.append(''.join(i.text.split()[k-1]))\n",
    "            r.append(''.join(i.text.split()[k]))\n",
    "        z=z+1\n",
    "              \n",
    "s_no=[]\n",
    "for j in range(1,11):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df6c=pd.DataFrame({'All Rounder':p, 'Team':t, 'Rating':r}, index=s_no)\n",
    "print(\"Top 10 women’s ODI all-rounder along with the records of their team and rating.\")\n",
    "df6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb863f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 7. Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world : i) Headline ii) Timeiii) News Link\n",
    "\n",
    "page7=requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "page7   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup7= BeautifulSoup(page7.content)\n",
    "\n",
    "headline=[]                 #To store headlines\n",
    "for i in soup7.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    headline.append(i.get('title'))\n",
    "    \n",
    "time=[]                    #To store time\n",
    "for i in soup7.find_all('time',class_=\"LatestNews-timestamp\"):\n",
    "    time.append(i.text)\n",
    "\n",
    "url=[]                   #To store the urls\n",
    "for i in soup7.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    url.append(i.get('href'))\n",
    "\n",
    "l=len(headline)\n",
    "s_no=[]\n",
    "for j in range(1,l+1):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df7=pd.DataFrame({'Headline':headline, 'Time':time, 'News links':url}, index=s_no)\n",
    "print(\"News details\")\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95ad0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8. Write a python program to scrape the details of most downloaded articles from AI in last 90 days. https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details : i) Paper Title ii) Authors iii) Published Date iv) Paper URL\n",
    "\n",
    "page8=requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "page8   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup8= BeautifulSoup(page8.content)\n",
    "\n",
    "paper_title=[]           #To store the paper titles\n",
    "for i in soup8.find_all('h2',class_=\"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\"):\n",
    "    paper_title.append(i.text)\n",
    "\n",
    "authors=[]               #To store the author names\n",
    "for i in soup8.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    authors.append(i.text)\n",
    "    \n",
    "date=[]                  #To store the publication date\n",
    "for i in soup8.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    date.append(i.text)\n",
    "\n",
    "url=[]                   #To store the urls\n",
    "for i in soup8.find_all('a',class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    url.append(i.get('href'))\n",
    "\n",
    "l=len(paper_title)\n",
    "s_no=[]\n",
    "for j in range(1,l+1):\n",
    "    s_no.append(j)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df8=pd.DataFrame({'Paper Titles':paper_title, 'Authors':authors, 'Date of Publication':date, 'Paper Links':url}, index=s_no)\n",
    "print(\"The details of most downloaded articles from AI in last 90 days\")\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0e669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 9. Write a python program to scrape mentioned details from dineout.co.in : i) Restaurant name ii) Cuisine iii) Location iv) Ratings v) Image URL\n",
    "\n",
    "page9=requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "page9   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup9= BeautifulSoup(page9.content)\n",
    "\n",
    "r_name=[]           #To store the restaurant name\n",
    "for i in soup9.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    r_name.append(i.text)\n",
    "\n",
    "cuisine=[]           #To store the cuisine\n",
    "for i in soup9.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "    cuisine.append(' '.join(i.text.split()[6:]))\n",
    "cuisine\n",
    "\n",
    "location=[]         #To store location\n",
    "for i in soup9.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "location\n",
    "\n",
    "ratings=[]          #To store ratings\n",
    "for i in soup9.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text)\n",
    "\n",
    "images=[]           #to store images\n",
    "for i in soup9.find_all('img',class_=\"no-img\"):\n",
    "    images.append(i['data-src'])\n",
    "    \n",
    "images\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "df9=pd.DataFrame({'Restaurant Name':r_name, 'Cuisine':cuisine, 'Location':location, 'Ratings':ratings, 'Image URL':images})\n",
    "print(\"The details from dineout.co.in\")\n",
    "df9.set_index('Restaurant Name', inplace=True)\n",
    "df9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e47fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10. Write a python program to scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en i) Rank ii) Publication iii) h5-index iv) h5-median\n",
    "\n",
    "page10=requests.get(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")\n",
    "page10   #As the response is 200, we can proceed with this url\n",
    "\n",
    "soup10= BeautifulSoup(page10 .content)\n",
    "\n",
    "rank=[]        #To store rank\n",
    "for i in soup10.find_all('td',class_=\"gsc_mvt_p\"):\n",
    "    rank.append(i.text)\n",
    "\n",
    "p=[]           #To store publication\n",
    "for i in soup10.find_all('td',class_=\"gsc_mvt_t\"):\n",
    "    p.append(i.text)\n",
    "\n",
    "h5_i=[]        #To store h5 index\n",
    "for i in soup10.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_i.append(i.text)\n",
    "\n",
    "h5_m=[]        #To store h5 median\n",
    "for i in soup10.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_m.append(i.text)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)             #To display the entire dataframe\n",
    "df10=pd.DataFrame({'Rank':rank, 'Publication':p, 'h5 index':h5_i, 'h5 median':h5_m})\n",
    "print(\"The details of top publications from Google Scholar\")\n",
    "df10.set_index('Rank', inplace=True)\n",
    "df10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
